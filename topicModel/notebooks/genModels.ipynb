{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling on `data_schoolofinf`\n",
    "\n",
    "Using the scrapped metadata and downloaded PDF, we create the topic models.\n",
    "\n",
    "\n",
    "0. using `gensim`, create the corpus, vocabulary\n",
    "1. use `LDAtuning` (in R) to find the best number of topics avaiable\n",
    "2. create topic model using `gensim`\n",
    "3. visualise the results using `lda2vis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState(93748573)\n",
    "import os\n",
    "\n",
    "DATA_DIR = '../../data/data_schoolofinf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 11:43:55,588 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all the tokens together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_toks = pd.read_pickle(os.path.join(DATA_DIR,'toks', 'toks.combined.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>toks_metada</th>\n",
       "      <th>toks_pdf2txt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pub_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400818dc-63af-4a26-80c5-906f98e1f8ab</th>\n",
       "      <td>1989</td>\n",
       "      <td>[ballooning, stability, analysis, jet, hmode, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18b1a861-afef-4fff-bc80-d02e05be18c4</th>\n",
       "      <td>2013</td>\n",
       "      <td>[query, processing, data, integration, chapter...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      year  \\\n",
       "pub_id                                       \n",
       "400818dc-63af-4a26-80c5-906f98e1f8ab  1989   \n",
       "18b1a861-afef-4fff-bc80-d02e05be18c4  2013   \n",
       "\n",
       "                                                                            toks_metada  \\\n",
       "pub_id                                                                                    \n",
       "400818dc-63af-4a26-80c5-906f98e1f8ab  [ballooning, stability, analysis, jet, hmode, ...   \n",
       "18b1a861-afef-4fff-bc80-d02e05be18c4  [query, processing, data, integration, chapter...   \n",
       "\n",
       "                                     toks_pdf2txt  \n",
       "pub_id                                             \n",
       "400818dc-63af-4a26-80c5-906f98e1f8ab               \n",
       "18b1a861-afef-4fff-bc80-d02e05be18c4               "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_toks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_toks['toks'] = df_combined_toks.apply(\n",
    "    lambda row: list(row.toks_metada) + list(row.toks_pdf2txt), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using publications from 1997-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_toks = df_combined_toks.drop(\n",
    "    df_combined_toks[(df_combined_toks.year < 1997) | (df_combined_toks.year > 2017)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8028 entries, 18b1a861-afef-4fff-bc80-d02e05be18c4 to b2920a27-5293-4f4a-8874-4a0ea804d91a\n",
      "Data columns (total 3 columns):\n",
      "year            8028 non-null int64\n",
      "toks_metada     8028 non-null object\n",
      "toks_pdf2txt    8028 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 250.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined_toks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: metadata + PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 10:01:38,942 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-01-29 10:01:56,301 : INFO : built Dictionary(636191 unique tokens: ['query', 'processing', 'data', 'integration', 'chapter']...) from 8028 documents (total 25623592 corpus positions)\n",
      "2018-01-29 10:01:57,438 : INFO : discarding 581457 tokens: [('data', 4317), ('approach', 4177), ('based', 4816), ('access', 4192), ('model', 4714), ('system', 5123), ('paper', 4628), ('using', 4803), ('supervectors', 8), ('use', 4212)]...\n",
      "2018-01-29 10:01:57,439 : INFO : keeping 54734 tokens which were in no less than 10 and no more than 4014 (=50.0%) documents\n",
      "2018-01-29 10:01:57,665 : INFO : resulting dictionary: Dictionary(54734 unique tokens: ['query', 'processing', 'integration', 'chapter', 'illustrate']...)\n"
     ]
    }
   ],
   "source": [
    "docs = df_combined_toks.toks.tolist()\n",
    "\n",
    "combined_toks_dict = Dictionary(docs)\n",
    "\n",
    "# Filter to remove words thatappeared too frequent (in more than 50% of doucuments) \n",
    "# and too little (less than 10 occurences)\n",
    "combined_toks_dict.filter_extremes(no_below=10, no_above=.5)\n",
    "combined_toks_dict.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bow tagging for each publication:\n",
    "df_combined_toks['bow'] = df_combined_toks['toks'].apply(combined_toks_dict.doc2bow)\n",
    "\n",
    "# Generate a corpus based on the tokens, which we will be using later\n",
    "corpus = df_combined_toks.bow.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 10:16:18,510 : INFO : saving Dictionary object under ../../data/data_schoolofinf/corpora/dictionary.all, separately None\n",
      "2018-01-29 10:16:18,602 : INFO : saved ../../data/data_schoolofinf/corpora/dictionary.all\n"
     ]
    }
   ],
   "source": [
    "# Save corpus:\n",
    "combined_toks_dict.save(os.path.join(DATA_DIR, 'corpora','dictionary.all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tmp = df_combined_toks.toks\n",
    "no_pub = 0\n",
    "\n",
    "with open(os.path.join(DATA_DIR,'toks','toks2word.all'), 'w') as f:\n",
    "    for pub in tmp:\n",
    "        if len(pub):\n",
    "            out = \" \".join(pub)\n",
    "            f.write(out + \"\\n\")\n",
    "        else:\n",
    "            no_pub += 1\n",
    "            pass\n",
    "print(no_pub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: only Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = df_combined_toks[['year','toks_metada']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 11:24:10,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-01-29 11:24:11,490 : INFO : built Dictionary(26756 unique tokens: ['query', 'processing', 'data', 'integration', 'chapter']...) from 8028 documents (total 719861 corpus positions)\n",
      "2018-01-29 11:24:11,568 : INFO : discarding 20534 tokens: [('chase', 9), ('sv', 2), ('ubm', 2), ('supervectors', 2), ('synthesizer', 9), ('eer', 4), ('wsj', 7), ('rps', 2), ('openairinterface', 3), ('academia', 6)]...\n",
      "2018-01-29 11:24:11,569 : INFO : keeping 6222 tokens which were in no less than 10 and no more than 4014 (=50.0%) documents\n",
      "2018-01-29 11:24:11,583 : INFO : resulting dictionary: Dictionary(6222 unique tokens: ['query', 'processing', 'data', 'integration', 'chapter']...)\n"
     ]
    }
   ],
   "source": [
    "docs = df_metadata.toks_metada.tolist()\n",
    "\n",
    "toks_dict = Dictionary(docs)\n",
    "\n",
    "# Filter to remove words thatappeared too frequent (in more than 50% of doucuments) \n",
    "# and too little (less than 10 occurences)\n",
    "toks_dict.filter_extremes(no_below=10, no_above=.5)\n",
    "toks_dict.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bow tagging for each publication:\n",
    "df_metadata['bow'] = df_metadata['toks_metada'].apply(toks_dict.doc2bow)\n",
    "\n",
    "# Generate a corpus based on the tokens, which we will be using later\n",
    "corpus = df_metadata.bow.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 11:24:54,315 : INFO : saving Dictionary object under ../../data/data_schoolofinf/corpora/dictionary.meta, separately None\n",
      "2018-01-29 11:24:54,320 : INFO : saved ../../data/data_schoolofinf/corpora/dictionary.meta\n"
     ]
    }
   ],
   "source": [
    "# Save corpus:\n",
    "toks_dict.save(os.path.join(DATA_DIR, 'corpora','dictionary.meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tmp = df_metadata.toks_metada\n",
    "no_pub = 0\n",
    "\n",
    "with open(os.path.join(DATA_DIR,'toks','toks2word.meta'), 'w') as f:\n",
    "    for pub in tmp:\n",
    "        if len(pub):\n",
    "            out = \" \".join(pub)\n",
    "            f.write(out + \"\\n\")\n",
    "        else:\n",
    "            no_pub += 1\n",
    "            pass\n",
    "print(no_pub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict from 2012-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_toks = df_combined_toks.drop(\n",
    "    df_combined_toks[(df_combined_toks.year < 2012) | (df_combined_toks.year > 2017)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2013, 2012, 2014, 2016, 2015, 2017]\n"
     ]
    }
   ],
   "source": [
    "print(list(df_combined_toks.year.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3620 entries, 18b1a861-afef-4fff-bc80-d02e05be18c4 to b2920a27-5293-4f4a-8874-4a0ea804d91a\n",
      "Data columns (total 4 columns):\n",
      "year            3620 non-null int64\n",
      "toks_metada     3620 non-null object\n",
      "toks_pdf2txt    3620 non-null object\n",
      "toks            3620 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 141.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined_toks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: metadata + PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-29 11:44:45,810 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-01-29 11:45:01,056 : INFO : built Dictionary(376785 unique tokens: ['query', 'processing', 'data', 'integration', 'chapter']...) from 3620 documents (total 13084978 corpus positions)\n",
      "2018-01-29 11:45:01,974 : INFO : discarding 338843 tokens: [('data', 2339), ('part', 1843), ('problem', 1981), ('approach', 2225), ('based', 2528), ('access', 2347), ('second', 1869), ('model', 2398), ('system', 2553), ('informatik_germany', 8)]...\n",
      "2018-01-29 11:45:01,975 : INFO : keeping 37942 tokens which were in no less than 10 and no more than 1810 (=50.0%) documents\n",
      "2018-01-29 11:45:02,142 : INFO : resulting dictionary: Dictionary(37942 unique tokens: ['query', 'processing', 'integration', 'chapter', 'illustrate']...)\n"
     ]
    }
   ],
   "source": [
    "docs = df_combined_toks.toks.tolist()\n",
    "\n",
    "combined_toks_dict = Dictionary(docs)\n",
    "\n",
    "# Filter to remove words thatappeared too frequent (in more than 50% of doucuments) \n",
    "# and too little (less than 10 occurences)\n",
    "combined_toks_dict.filter_extremes(no_below=10, no_above=.5)\n",
    "combined_toks_dict.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bow tagging for each publication:\n",
    "df_combined_toks['bow'] = df_combined_toks['toks'].apply(combined_toks_dict.doc2bow)\n",
    "\n",
    "# Generate a corpus based on the tokens, which we will be using later\n",
    "corpus = df_combined_toks.bow.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tmp = df_combined_toks.toks\n",
    "no_pub = 0\n",
    "\n",
    "with open(os.path.join(DATA_DIR,'toks','toks2word.less.all'), 'w') as f:\n",
    "    for pub in tmp:\n",
    "        if len(pub):\n",
    "            out = \" \".join(pub)\n",
    "            f.write(out + \"\\n\")\n",
    "        else:\n",
    "            no_pub += 1\n",
    "            pass\n",
    "print(no_pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
