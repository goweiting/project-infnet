{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling on `data_schoolofinf`\n",
    "\n",
    "Using the scrapped metadata and downloaded PDF, we create the topic models.\n",
    "\n",
    "\n",
    "0. using `gensim`, create the corpus, vocabulary\n",
    "1. use `LDAtuning` (in R) to find the best number of topics avaiable\n",
    "2. create topic model using `gensim`\n",
    "3. visualise the results using `lda2vis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:03:31.181944Z",
     "start_time": "2018-02-01T00:03:30.785084Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState(93748573)\n",
    "import os\n",
    "\n",
    "DATA_DIR = '../../data/data_schoolofinf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:03:31.714887Z",
     "start_time": "2018-02-01T00:03:31.183749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-01 00:03:31,707 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all the tokens together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T23:53:21.806509Z",
     "start_time": "2018-01-31T23:53:16.203938Z"
    }
   },
   "outputs": [],
   "source": [
    "df_combined_toks = pd.read_pickle(os.path.join(DATA_DIR,'toks', 'toks.combined.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T23:53:21.905010Z",
     "start_time": "2018-01-31T23:53:21.808569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>toks_metada</th>\n",
       "      <th>toks_pdf2txt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pub_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400818dc-63af-4a26-80c5-906f98e1f8ab</th>\n",
       "      <td>1989</td>\n",
       "      <td>[ballooning, stability, analysis, jet, hmode, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18b1a861-afef-4fff-bc80-d02e05be18c4</th>\n",
       "      <td>2013</td>\n",
       "      <td>[query, processing, data, integration, chapter...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      year  \\\n",
       "pub_id                                       \n",
       "400818dc-63af-4a26-80c5-906f98e1f8ab  1989   \n",
       "18b1a861-afef-4fff-bc80-d02e05be18c4  2013   \n",
       "\n",
       "                                                                            toks_metada  \\\n",
       "pub_id                                                                                    \n",
       "400818dc-63af-4a26-80c5-906f98e1f8ab  [ballooning, stability, analysis, jet, hmode, ...   \n",
       "18b1a861-afef-4fff-bc80-d02e05be18c4  [query, processing, data, integration, chapter...   \n",
       "\n",
       "                                     toks_pdf2txt  \n",
       "pub_id                                             \n",
       "400818dc-63af-4a26-80c5-906f98e1f8ab               \n",
       "18b1a861-afef-4fff-bc80-d02e05be18c4               "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_toks.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T23:53:23.584587Z",
     "start_time": "2018-01-31T23:53:21.907727Z"
    }
   },
   "outputs": [],
   "source": [
    "df_combined_toks['toks'] = df_combined_toks.apply(\n",
    "    lambda row: list(row.toks_metada) + list(row.toks_pdf2txt), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using publications from 1997-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T23:53:23.607793Z",
     "start_time": "2018-01-31T23:53:23.586699Z"
    }
   },
   "outputs": [],
   "source": [
    "df_combined_toks = df_combined_toks.drop(\n",
    "    df_combined_toks[(df_combined_toks.year < 1997) | (df_combined_toks.year > 2017)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T23:53:23.639024Z",
     "start_time": "2018-01-31T23:53:23.610752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8028 entries, 18b1a861-afef-4fff-bc80-d02e05be18c4 to b2920a27-5293-4f4a-8874-4a0ea804d91a\n",
      "Data columns (total 4 columns):\n",
      "year            8028 non-null int64\n",
      "toks_metada     8028 non-null object\n",
      "toks_pdf2txt    8028 non-null object\n",
      "toks            8028 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 313.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined_toks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: metadata + PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T23:53:42.700540Z",
     "start_time": "2018-01-31T23:53:23.641623Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 23:53:23,656 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-01-31 23:53:41,514 : INFO : built Dictionary(636191 unique tokens: ['query', 'processing', 'data', 'integration', 'chapter']...) from 8028 documents (total 25623592 corpus positions)\n",
      "2018-01-31 23:53:42,445 : INFO : discarding 581457 tokens: [('data', 4317), ('approach', 4177), ('based', 4816), ('access', 4192), ('model', 4714), ('system', 5123), ('paper', 4628), ('using', 4803), ('supervectors', 8), ('use', 4212)]...\n",
      "2018-01-31 23:53:42,446 : INFO : keeping 54734 tokens which were in no less than 10 and no more than 4014 (=50.0%) documents\n",
      "2018-01-31 23:53:42,633 : INFO : resulting dictionary: Dictionary(54734 unique tokens: ['query', 'processing', 'integration', 'chapter', 'illustrate']...)\n"
     ]
    }
   ],
   "source": [
    "docs = df_combined_toks.toks.tolist()\n",
    "\n",
    "combined_toks_dict = Dictionary(docs)\n",
    "\n",
    "# Filter to remove words thatappeared too frequent (in more than 50% of doucuments) \n",
    "# and too little (less than 10 occurences)\n",
    "combined_toks_dict.filter_extremes(no_below=10, no_above=.5)\n",
    "combined_toks_dict.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a bow tagging for each publication:\n",
    "# df_combined_toks['bow'] = df_combined_toks['toks'].apply(combined_toks_dict.doc2bow)\n",
    "\n",
    "# # Generate a corpus based on the tokens, which we will be using later\n",
    "# corpus = df_combined_toks.bow.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T23:00:46.843983Z",
     "start_time": "2018-01-31T23:00:46.786078Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 23:00:46,788 : INFO : saving Dictionary object under ../../data/data_schoolofinf/corpora/dictionary.all, separately None\n",
      "2018-01-31 23:00:46,840 : INFO : saved ../../data/data_schoolofinf/corpora/dictionary.all\n"
     ]
    }
   ],
   "source": [
    "# Save corpus:\n",
    "combined_toks_dict.save(os.path.join(DATA_DIR, 'corpora','dictionary.all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:00:04.916079Z",
     "start_time": "2018-01-31T23:59:48.062284Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, 'toks', 'bow2idx.all'), 'w') as f:\n",
    "    for pub in docs:\n",
    "        bow = combined_toks_dict.doc2bow(pub)# list of (id,count)\n",
    "        if len(bow):\n",
    "            rep = \"\".join([(str(a)+\" \")*c for (a, c) in bow])\n",
    "            f.write(rep.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: only Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:00:38.178112Z",
     "start_time": "2018-02-01T00:00:38.167703Z"
    }
   },
   "outputs": [],
   "source": [
    "df_metadata = df_combined_toks[['year','toks_metada']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:00:39.104644Z",
     "start_time": "2018-02-01T00:00:38.180933Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-01 00:00:38,208 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-02-01 00:00:39,055 : INFO : built Dictionary(26756 unique tokens: ['query', 'processing', 'data', 'integration', 'chapter']...) from 8028 documents (total 719861 corpus positions)\n",
      "2018-02-01 00:00:39,089 : INFO : discarding 20534 tokens: [('chase', 9), ('sv', 2), ('ubm', 2), ('supervectors', 2), ('synthesizer', 9), ('eer', 4), ('wsj', 7), ('rps', 2), ('openairinterface', 3), ('academia', 6)]...\n",
      "2018-02-01 00:00:39,090 : INFO : keeping 6222 tokens which were in no less than 10 and no more than 4014 (=50.0%) documents\n",
      "2018-02-01 00:00:39,099 : INFO : resulting dictionary: Dictionary(6222 unique tokens: ['query', 'processing', 'data', 'integration', 'chapter']...)\n"
     ]
    }
   ],
   "source": [
    "docs = df_metadata.toks_metada.tolist()\n",
    "\n",
    "toks_dict = Dictionary(docs)\n",
    "\n",
    "# Filter to remove words thatappeared too frequent (in more than 50% of doucuments) \n",
    "# and too little (less than 10 occurences)\n",
    "toks_dict.filter_extremes(no_below=10, no_above=.5)\n",
    "toks_dict.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:00:39.108822Z",
     "start_time": "2018-02-01T00:00:39.106344Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create a bow tagging for each publication:\n",
    "# df_metadata['bow'] = df_metadata['toks_metada'].apply(toks_dict.doc2bow)\n",
    "\n",
    "# # Generate a corpus based on the tokens, which we will be using later\n",
    "# corpus = df_metadata.bow.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:00:39.141327Z",
     "start_time": "2018-02-01T00:00:39.110501Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-01 00:00:39,135 : INFO : saving Dictionary object under ../../data/data_schoolofinf/corpora/dictionary.meta, separately None\n",
      "2018-02-01 00:00:39,139 : INFO : saved ../../data/data_schoolofinf/corpora/dictionary.meta\n"
     ]
    }
   ],
   "source": [
    "# Save corpus:\n",
    "toks_dict.save(os.path.join(DATA_DIR, 'corpora','dictionary.meta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:00:39.966701Z",
     "start_time": "2018-02-01T00:00:39.143015Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, 'toks', 'bow2idx.meta'), 'w') as f:\n",
    "    for pub in docs:\n",
    "        bow = toks_dict.doc2bow(pub)\n",
    "        if len(bow):\n",
    "            rep = \"\".join([(str(a)+\" \")*c for (a, c) in bow])\n",
    "            f.write(rep.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict from 2012-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:00:56.599966Z",
     "start_time": "2018-02-01T00:00:56.582239Z"
    }
   },
   "outputs": [],
   "source": [
    "df_combined_toks = df_combined_toks.drop(\n",
    "    df_combined_toks[(df_combined_toks.year < 2012) | (df_combined_toks.year > 2017)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:00:56.622733Z",
     "start_time": "2018-02-01T00:00:56.602566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2013, 2012, 2014, 2016, 2015, 2017]\n"
     ]
    }
   ],
   "source": [
    "print(list(df_combined_toks.year.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:00:56.649295Z",
     "start_time": "2018-02-01T00:00:56.625245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3620 entries, 18b1a861-afef-4fff-bc80-d02e05be18c4 to b2920a27-5293-4f4a-8874-4a0ea804d91a\n",
      "Data columns (total 4 columns):\n",
      "year            3620 non-null int64\n",
      "toks_metada     3620 non-null object\n",
      "toks_pdf2txt    3620 non-null object\n",
      "toks            3620 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 141.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined_toks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: metadata + PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:01:06.494882Z",
     "start_time": "2018-02-01T00:00:56.651744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-01 00:00:56,661 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-02-01 00:01:05,770 : INFO : built Dictionary(376785 unique tokens: ['query', 'processing', 'data', 'integration', 'chapter']...) from 3620 documents (total 13084978 corpus positions)\n",
      "2018-02-01 00:01:06,334 : INFO : discarding 338843 tokens: [('data', 2339), ('part', 1843), ('problem', 1981), ('approach', 2225), ('based', 2528), ('access', 2347), ('second', 1869), ('model', 2398), ('system', 2553), ('informatik_germany', 8)]...\n",
      "2018-02-01 00:01:06,335 : INFO : keeping 37942 tokens which were in no less than 10 and no more than 1810 (=50.0%) documents\n",
      "2018-02-01 00:01:06,452 : INFO : resulting dictionary: Dictionary(37942 unique tokens: ['query', 'processing', 'integration', 'chapter', 'illustrate']...)\n"
     ]
    }
   ],
   "source": [
    "docs = df_combined_toks.toks.tolist()\n",
    "\n",
    "combined_toks_dict = Dictionary(docs)\n",
    "\n",
    "# Filter to remove words thatappeared too frequent (in more than 50% of doucuments) \n",
    "# and too little (less than 10 occurences)\n",
    "combined_toks_dict.filter_extremes(no_below=10, no_above=.5)\n",
    "combined_toks_dict.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:01:06.500809Z",
     "start_time": "2018-02-01T00:01:06.497296Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create a bow tagging for each publication:\n",
    "# df_combined_toks['bow'] = df_combined_toks['toks'].apply(combined_toks_dict.doc2bow)\n",
    "\n",
    "# # Generate a corpus based on the tokens, which we will be using later\n",
    "# corpus = df_combined_toks.bow.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:01:06.546333Z",
     "start_time": "2018-02-01T00:01:06.502506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-01 00:01:06,525 : INFO : saving Dictionary object under ../../data/data_schoolofinf/corpora/dictionary.less.all, separately None\n",
      "2018-02-01 00:01:06,544 : INFO : saved ../../data/data_schoolofinf/corpora/dictionary.less.all\n"
     ]
    }
   ],
   "source": [
    "# Save corpus:\n",
    "combined_toks_dict.save(os.path.join(DATA_DIR, 'corpora','dictionary.less.all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:01:14.725592Z",
     "start_time": "2018-02-01T00:01:06.548315Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, 'toks', 'bow2idx.less.all'), 'w') as f:\n",
    "    for pub in docs:\n",
    "        bow = combined_toks_dict.doc2bow(pub)\n",
    "        if len(bow):\n",
    "            rep = \"\".join([(str(a)+\" \")*c for (a, c) in bow])\n",
    "            f.write(rep.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic models for dblp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:03:41.888017Z",
     "start_time": "2018-02-01T00:03:41.882352Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/data_dblp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:06:31.160703Z",
     "start_time": "2018-02-01T00:05:08.490489Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dblp = pd.read_pickle(os.path.join(DATA_DIR,'toks','toks.dblp.1997-2017.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:06:31.181624Z",
     "start_time": "2018-02-01T00:06:31.162652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>toks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00745041-3636-4d18-bbec-783c4278c40d</th>\n",
       "      <td>2003</td>\n",
       "      <td>[self, stabilizing, algorithm, finding, cuttin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00dc2bba-3237-4d4e-b541-1205b97df981</th>\n",
       "      <td>2003</td>\n",
       "      <td>[software, evolution, transformation, electron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04136c62-06a9-4c74-9da2-547448a9dc6f</th>\n",
       "      <td>2003</td>\n",
       "      <td>[kernel, pls, variant, regression, european, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      year  \\\n",
       "id                                           \n",
       "00745041-3636-4d18-bbec-783c4278c40d  2003   \n",
       "00dc2bba-3237-4d4e-b541-1205b97df981  2003   \n",
       "04136c62-06a9-4c74-9da2-547448a9dc6f  2003   \n",
       "\n",
       "                                                                                   toks  \n",
       "id                                                                                       \n",
       "00745041-3636-4d18-bbec-783c4278c40d  [self, stabilizing, algorithm, finding, cuttin...  \n",
       "00dc2bba-3237-4d4e-b541-1205b97df981  [software, evolution, transformation, electron...  \n",
       "04136c62-06a9-4c74-9da2-547448a9dc6f  [kernel, pls, variant, regression, european, s...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dblp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T21:37:22.440489Z",
     "start_time": "2018-01-31T21:30:28.074639Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 21:30:31,153 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-01-31 21:30:31,688 : INFO : adding document #10000 to Dictionary(33842 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:32,698 : INFO : adding document #20000 to Dictionary(58906 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:33,718 : INFO : adding document #30000 to Dictionary(77373 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:34,985 : INFO : adding document #40000 to Dictionary(96811 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:36,217 : INFO : adding document #50000 to Dictionary(112050 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:37,546 : INFO : adding document #60000 to Dictionary(125243 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:38,995 : INFO : adding document #70000 to Dictionary(136485 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:40,465 : INFO : adding document #80000 to Dictionary(146403 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:41,469 : INFO : adding document #90000 to Dictionary(155961 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:42,564 : INFO : adding document #100000 to Dictionary(165625 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:43,800 : INFO : adding document #110000 to Dictionary(176388 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:45,363 : INFO : adding document #120000 to Dictionary(186991 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:47,145 : INFO : adding document #130000 to Dictionary(197196 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:48,670 : INFO : adding document #140000 to Dictionary(206165 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:50,497 : INFO : adding document #150000 to Dictionary(214535 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:52,080 : INFO : adding document #160000 to Dictionary(222231 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:53,455 : INFO : adding document #170000 to Dictionary(229315 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:54,866 : INFO : adding document #180000 to Dictionary(235906 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:56,653 : INFO : adding document #190000 to Dictionary(242052 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:58,096 : INFO : adding document #200000 to Dictionary(248104 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:30:59,714 : INFO : adding document #210000 to Dictionary(253500 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:01,428 : INFO : adding document #220000 to Dictionary(258801 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:03,000 : INFO : adding document #230000 to Dictionary(263867 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:04,605 : INFO : adding document #240000 to Dictionary(269335 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:06,639 : INFO : adding document #250000 to Dictionary(274724 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:08,415 : INFO : adding document #260000 to Dictionary(279862 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:09,885 : INFO : adding document #270000 to Dictionary(284916 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:10,811 : INFO : adding document #280000 to Dictionary(290471 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:11,434 : INFO : adding document #290000 to Dictionary(294360 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:12,128 : INFO : adding document #300000 to Dictionary(298387 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:13,288 : INFO : adding document #310000 to Dictionary(303334 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:14,427 : INFO : adding document #320000 to Dictionary(307529 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:15,862 : INFO : adding document #330000 to Dictionary(312170 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:17,622 : INFO : adding document #340000 to Dictionary(316379 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:19,162 : INFO : adding document #350000 to Dictionary(320592 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:20,814 : INFO : adding document #360000 to Dictionary(324621 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:22,282 : INFO : adding document #370000 to Dictionary(328567 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:23,784 : INFO : adding document #380000 to Dictionary(332331 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:25,287 : INFO : adding document #390000 to Dictionary(336007 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:26,699 : INFO : adding document #400000 to Dictionary(339829 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:28,104 : INFO : adding document #410000 to Dictionary(343516 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:29,566 : INFO : adding document #420000 to Dictionary(347016 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:31,205 : INFO : adding document #430000 to Dictionary(350322 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:32,809 : INFO : adding document #440000 to Dictionary(353575 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:34,622 : INFO : adding document #450000 to Dictionary(356907 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:36,120 : INFO : adding document #460000 to Dictionary(360071 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:37,346 : INFO : adding document #470000 to Dictionary(364360 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:38,540 : INFO : adding document #480000 to Dictionary(367449 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:39,649 : INFO : adding document #490000 to Dictionary(370470 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:40,949 : INFO : adding document #500000 to Dictionary(373964 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:42,376 : INFO : adding document #510000 to Dictionary(377656 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:44,062 : INFO : adding document #520000 to Dictionary(380881 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:45,576 : INFO : adding document #530000 to Dictionary(383935 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 21:31:47,162 : INFO : adding document #540000 to Dictionary(387080 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:48,761 : INFO : adding document #550000 to Dictionary(390017 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:50,203 : INFO : adding document #560000 to Dictionary(392926 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:51,792 : INFO : adding document #570000 to Dictionary(395888 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:53,305 : INFO : adding document #580000 to Dictionary(398865 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:54,720 : INFO : adding document #590000 to Dictionary(401849 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:56,245 : INFO : adding document #600000 to Dictionary(404978 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:57,868 : INFO : adding document #610000 to Dictionary(407872 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:31:59,391 : INFO : adding document #620000 to Dictionary(410680 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:01,250 : INFO : adding document #630000 to Dictionary(413367 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:03,066 : INFO : adding document #640000 to Dictionary(416533 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:04,677 : INFO : adding document #650000 to Dictionary(419857 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:06,200 : INFO : adding document #660000 to Dictionary(423091 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:07,782 : INFO : adding document #670000 to Dictionary(426340 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:08,480 : INFO : adding document #680000 to Dictionary(429044 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:08,980 : INFO : adding document #690000 to Dictionary(431200 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:09,860 : INFO : adding document #700000 to Dictionary(435325 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:11,027 : INFO : adding document #710000 to Dictionary(438915 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:12,248 : INFO : adding document #720000 to Dictionary(442603 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:13,933 : INFO : adding document #730000 to Dictionary(445642 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:14,853 : INFO : adding document #740000 to Dictionary(448586 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:16,057 : INFO : adding document #750000 to Dictionary(451581 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:17,536 : INFO : adding document #760000 to Dictionary(455517 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:18,764 : INFO : adding document #770000 to Dictionary(458450 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:20,180 : INFO : adding document #780000 to Dictionary(460875 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:21,800 : INFO : adding document #790000 to Dictionary(463890 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:23,529 : INFO : adding document #800000 to Dictionary(466909 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:25,345 : INFO : adding document #810000 to Dictionary(470007 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:27,294 : INFO : adding document #820000 to Dictionary(472896 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:28,602 : INFO : adding document #830000 to Dictionary(475362 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:30,041 : INFO : adding document #840000 to Dictionary(477773 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:31,382 : INFO : adding document #850000 to Dictionary(480275 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:32,278 : INFO : adding document #860000 to Dictionary(481943 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:33,183 : INFO : adding document #870000 to Dictionary(484757 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:34,504 : INFO : adding document #880000 to Dictionary(487608 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:35,931 : INFO : adding document #890000 to Dictionary(490394 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:37,676 : INFO : adding document #900000 to Dictionary(493606 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:39,337 : INFO : adding document #910000 to Dictionary(496040 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:41,047 : INFO : adding document #920000 to Dictionary(498224 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:42,447 : INFO : adding document #930000 to Dictionary(500515 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:43,335 : INFO : adding document #940000 to Dictionary(503125 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:44,342 : INFO : adding document #950000 to Dictionary(505313 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:45,766 : INFO : adding document #960000 to Dictionary(508681 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:47,175 : INFO : adding document #970000 to Dictionary(511696 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:48,949 : INFO : adding document #980000 to Dictionary(513988 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:50,678 : INFO : adding document #990000 to Dictionary(516226 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:52,290 : INFO : adding document #1000000 to Dictionary(518726 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:53,828 : INFO : adding document #1010000 to Dictionary(521056 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:55,273 : INFO : adding document #1020000 to Dictionary(523494 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:56,780 : INFO : adding document #1030000 to Dictionary(525864 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:58,176 : INFO : adding document #1040000 to Dictionary(528288 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:32:59,884 : INFO : adding document #1050000 to Dictionary(530516 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:01,541 : INFO : adding document #1060000 to Dictionary(532993 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 21:33:03,293 : INFO : adding document #1070000 to Dictionary(535314 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:04,820 : INFO : adding document #1080000 to Dictionary(537610 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:06,291 : INFO : adding document #1090000 to Dictionary(539797 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:08,079 : INFO : adding document #1100000 to Dictionary(542625 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:09,707 : INFO : adding document #1110000 to Dictionary(545907 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:11,429 : INFO : adding document #1120000 to Dictionary(548859 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:12,948 : INFO : adding document #1130000 to Dictionary(551508 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:14,160 : INFO : adding document #1140000 to Dictionary(553774 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:15,014 : INFO : adding document #1150000 to Dictionary(555546 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:15,791 : INFO : adding document #1160000 to Dictionary(557358 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:16,837 : INFO : adding document #1170000 to Dictionary(560001 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:18,006 : INFO : adding document #1180000 to Dictionary(561942 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:19,453 : INFO : adding document #1190000 to Dictionary(564149 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:21,183 : INFO : adding document #1200000 to Dictionary(566754 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:22,804 : INFO : adding document #1210000 to Dictionary(569117 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:24,275 : INFO : adding document #1220000 to Dictionary(571465 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:25,629 : INFO : adding document #1230000 to Dictionary(573887 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:27,027 : INFO : adding document #1240000 to Dictionary(576284 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:28,618 : INFO : adding document #1250000 to Dictionary(578706 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:29,950 : INFO : adding document #1260000 to Dictionary(581146 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:31,576 : INFO : adding document #1270000 to Dictionary(583600 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:33,121 : INFO : adding document #1280000 to Dictionary(585740 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:34,668 : INFO : adding document #1290000 to Dictionary(588027 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:36,230 : INFO : adding document #1300000 to Dictionary(590219 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:37,818 : INFO : adding document #1310000 to Dictionary(592404 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:39,260 : INFO : adding document #1320000 to Dictionary(594726 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:40,277 : INFO : adding document #1330000 to Dictionary(596736 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:41,449 : INFO : adding document #1340000 to Dictionary(599455 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:42,717 : INFO : adding document #1350000 to Dictionary(601874 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:43,992 : INFO : adding document #1360000 to Dictionary(604725 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:45,478 : INFO : adding document #1370000 to Dictionary(606858 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:47,250 : INFO : adding document #1380000 to Dictionary(608778 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:48,495 : INFO : adding document #1390000 to Dictionary(610615 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:50,093 : INFO : adding document #1400000 to Dictionary(613144 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:51,579 : INFO : adding document #1410000 to Dictionary(615337 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:53,386 : INFO : adding document #1420000 to Dictionary(617234 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:55,141 : INFO : adding document #1430000 to Dictionary(619092 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:56,814 : INFO : adding document #1440000 to Dictionary(621198 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:33:58,477 : INFO : adding document #1450000 to Dictionary(623610 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:00,272 : INFO : adding document #1460000 to Dictionary(625832 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:02,018 : INFO : adding document #1470000 to Dictionary(628056 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:03,841 : INFO : adding document #1480000 to Dictionary(630328 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:05,862 : INFO : adding document #1490000 to Dictionary(632598 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:07,654 : INFO : adding document #1500000 to Dictionary(634848 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:09,642 : INFO : adding document #1510000 to Dictionary(637016 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:11,513 : INFO : adding document #1520000 to Dictionary(639553 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:13,484 : INFO : adding document #1530000 to Dictionary(642261 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:15,133 : INFO : adding document #1540000 to Dictionary(644741 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:16,858 : INFO : adding document #1550000 to Dictionary(646932 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:18,713 : INFO : adding document #1560000 to Dictionary(649453 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:19,982 : INFO : adding document #1570000 to Dictionary(651628 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:21,306 : INFO : adding document #1580000 to Dictionary(653524 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:22,307 : INFO : adding document #1590000 to Dictionary(655330 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 21:34:23,397 : INFO : adding document #1600000 to Dictionary(656991 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:24,196 : INFO : adding document #1610000 to Dictionary(658353 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:24,842 : INFO : adding document #1620000 to Dictionary(660006 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:25,655 : INFO : adding document #1630000 to Dictionary(661852 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:26,831 : INFO : adding document #1640000 to Dictionary(663729 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:28,075 : INFO : adding document #1650000 to Dictionary(665636 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:29,633 : INFO : adding document #1660000 to Dictionary(667880 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:31,148 : INFO : adding document #1670000 to Dictionary(670120 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:32,668 : INFO : adding document #1680000 to Dictionary(672271 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:34,261 : INFO : adding document #1690000 to Dictionary(674002 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:35,938 : INFO : adding document #1700000 to Dictionary(675860 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:37,461 : INFO : adding document #1710000 to Dictionary(677723 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:38,642 : INFO : adding document #1720000 to Dictionary(679388 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:39,328 : INFO : adding document #1730000 to Dictionary(681041 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:40,668 : INFO : adding document #1740000 to Dictionary(682680 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:41,877 : INFO : adding document #1750000 to Dictionary(684251 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:43,413 : INFO : adding document #1760000 to Dictionary(686228 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:44,775 : INFO : adding document #1770000 to Dictionary(688168 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:46,076 : INFO : adding document #1780000 to Dictionary(690307 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:47,525 : INFO : adding document #1790000 to Dictionary(692252 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:48,990 : INFO : adding document #1800000 to Dictionary(693980 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:50,504 : INFO : adding document #1810000 to Dictionary(695670 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:52,038 : INFO : adding document #1820000 to Dictionary(697167 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:53,838 : INFO : adding document #1830000 to Dictionary(698898 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:55,103 : INFO : adding document #1840000 to Dictionary(700484 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:56,022 : INFO : adding document #1850000 to Dictionary(702874 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:57,202 : INFO : adding document #1860000 to Dictionary(705754 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:58,340 : INFO : adding document #1870000 to Dictionary(708236 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:34:59,347 : INFO : adding document #1880000 to Dictionary(709977 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:00,168 : INFO : adding document #1890000 to Dictionary(711994 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:01,727 : INFO : adding document #1900000 to Dictionary(713955 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:03,008 : INFO : adding document #1910000 to Dictionary(715859 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:04,389 : INFO : adding document #1920000 to Dictionary(717952 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:05,852 : INFO : adding document #1930000 to Dictionary(719802 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:07,351 : INFO : adding document #1940000 to Dictionary(721761 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:08,794 : INFO : adding document #1950000 to Dictionary(723683 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:10,336 : INFO : adding document #1960000 to Dictionary(725495 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:11,925 : INFO : adding document #1970000 to Dictionary(727306 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:13,457 : INFO : adding document #1980000 to Dictionary(728932 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:14,994 : INFO : adding document #1990000 to Dictionary(730788 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:16,812 : INFO : adding document #2000000 to Dictionary(732582 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:18,551 : INFO : adding document #2010000 to Dictionary(734319 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:20,181 : INFO : adding document #2020000 to Dictionary(736063 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:21,298 : INFO : adding document #2030000 to Dictionary(737834 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:22,986 : INFO : adding document #2040000 to Dictionary(739603 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:25,069 : INFO : adding document #2050000 to Dictionary(741689 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:27,138 : INFO : adding document #2060000 to Dictionary(743710 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:28,871 : INFO : adding document #2070000 to Dictionary(745825 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:30,657 : INFO : adding document #2080000 to Dictionary(747908 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:32,329 : INFO : adding document #2090000 to Dictionary(749913 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:33,777 : INFO : adding document #2100000 to Dictionary(751799 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:35,198 : INFO : adding document #2110000 to Dictionary(753921 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:36,989 : INFO : adding document #2120000 to Dictionary(756137 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 21:35:38,632 : INFO : adding document #2130000 to Dictionary(758067 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:40,011 : INFO : adding document #2140000 to Dictionary(759901 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:41,261 : INFO : adding document #2150000 to Dictionary(761609 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:42,574 : INFO : adding document #2160000 to Dictionary(763312 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:43,993 : INFO : adding document #2170000 to Dictionary(765049 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:45,284 : INFO : adding document #2180000 to Dictionary(766636 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:46,731 : INFO : adding document #2190000 to Dictionary(768555 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:48,144 : INFO : adding document #2200000 to Dictionary(770467 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:49,424 : INFO : adding document #2210000 to Dictionary(772198 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:50,709 : INFO : adding document #2220000 to Dictionary(774407 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:52,203 : INFO : adding document #2230000 to Dictionary(776168 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:53,193 : INFO : adding document #2240000 to Dictionary(777816 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:54,620 : INFO : adding document #2250000 to Dictionary(779470 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:55,629 : INFO : adding document #2260000 to Dictionary(781217 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:56,692 : INFO : adding document #2270000 to Dictionary(782905 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:58,157 : INFO : adding document #2280000 to Dictionary(784785 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:35:59,677 : INFO : adding document #2290000 to Dictionary(786854 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:01,460 : INFO : adding document #2300000 to Dictionary(788678 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:03,102 : INFO : adding document #2310000 to Dictionary(790443 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:04,598 : INFO : adding document #2320000 to Dictionary(792063 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:05,998 : INFO : adding document #2330000 to Dictionary(794156 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:07,701 : INFO : adding document #2340000 to Dictionary(795893 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:09,323 : INFO : adding document #2350000 to Dictionary(797433 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:10,863 : INFO : adding document #2360000 to Dictionary(799108 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:12,494 : INFO : adding document #2370000 to Dictionary(800663 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:14,045 : INFO : adding document #2380000 to Dictionary(802264 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:15,824 : INFO : adding document #2390000 to Dictionary(803710 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:17,411 : INFO : adding document #2400000 to Dictionary(805293 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:19,012 : INFO : adding document #2410000 to Dictionary(806837 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:20,724 : INFO : adding document #2420000 to Dictionary(808258 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:22,742 : INFO : adding document #2430000 to Dictionary(809674 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:24,480 : INFO : adding document #2440000 to Dictionary(811377 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:25,089 : INFO : adding document #2450000 to Dictionary(813030 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:25,716 : INFO : adding document #2460000 to Dictionary(814307 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:26,752 : INFO : adding document #2470000 to Dictionary(816032 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:28,177 : INFO : adding document #2480000 to Dictionary(817444 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:29,500 : INFO : adding document #2490000 to Dictionary(818992 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:30,987 : INFO : adding document #2500000 to Dictionary(820687 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:32,567 : INFO : adding document #2510000 to Dictionary(822383 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:34,088 : INFO : adding document #2520000 to Dictionary(824027 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:35,467 : INFO : adding document #2530000 to Dictionary(825799 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:36,959 : INFO : adding document #2540000 to Dictionary(827293 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:38,301 : INFO : adding document #2550000 to Dictionary(828596 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:39,916 : INFO : adding document #2560000 to Dictionary(830152 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:41,340 : INFO : adding document #2570000 to Dictionary(831629 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:42,927 : INFO : adding document #2580000 to Dictionary(833032 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:44,050 : INFO : adding document #2590000 to Dictionary(834296 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:44,900 : INFO : adding document #2600000 to Dictionary(836302 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:45,978 : INFO : adding document #2610000 to Dictionary(838134 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:47,271 : INFO : adding document #2620000 to Dictionary(840098 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:48,690 : INFO : adding document #2630000 to Dictionary(841719 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:49,962 : INFO : adding document #2640000 to Dictionary(843142 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:50,976 : INFO : adding document #2650000 to Dictionary(844591 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 21:36:51,852 : INFO : adding document #2660000 to Dictionary(846412 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:53,169 : INFO : adding document #2670000 to Dictionary(848269 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:54,602 : INFO : adding document #2680000 to Dictionary(850496 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:56,162 : INFO : adding document #2690000 to Dictionary(852282 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:57,684 : INFO : adding document #2700000 to Dictionary(854102 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:36:59,478 : INFO : adding document #2710000 to Dictionary(855910 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:01,116 : INFO : adding document #2720000 to Dictionary(857565 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:02,932 : INFO : adding document #2730000 to Dictionary(859277 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:04,393 : INFO : adding document #2740000 to Dictionary(861022 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:05,878 : INFO : adding document #2750000 to Dictionary(862635 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:07,690 : INFO : adding document #2760000 to Dictionary(864155 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:09,252 : INFO : adding document #2770000 to Dictionary(865589 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:10,711 : INFO : adding document #2780000 to Dictionary(867232 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:12,441 : INFO : adding document #2790000 to Dictionary(868805 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:14,400 : INFO : adding document #2800000 to Dictionary(870328 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n",
      "2018-01-31 21:37:14,791 : INFO : built Dictionary(871028 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...) from 2805097 documents (total 272983767 corpus positions)\n",
      "2018-01-31 21:37:21,679 : INFO : discarding 794557 tokens: [('we', 942418), ('flnding', 17), ('princi', 36), ('flts', 10), ('near_synonym', 35), ('smartkom', 33), ('mersenne_number', 34), ('system', 1088258), ('using', 844586), ('multitransfer', 1)]...\n",
      "2018-01-31 21:37:21,681 : INFO : keeping 76471 tokens which were in no less than 50 and no more than 841529 (=30.0%) documents\n",
      "2018-01-31 21:37:22,285 : INFO : resulting dictionary: Dictionary(76471 unique tokens: ['self', 'stabilizing', 'algorithm', 'finding', 'cutting']...)\n"
     ]
    }
   ],
   "source": [
    "docs_dblp = df_dblp.toks.tolist()\n",
    "dict_dblp = Dictionary(docs_dblp)\n",
    "\n",
    "# Using a stricter rule here!\n",
    "# Filter to remove words thatappeared too frequent (in more than 50% of doucuments) \n",
    "# and too little (less than 10 occurences)\n",
    "dict_dblp.filter_extremes(no_below=50, no_above=.3)\n",
    "dict_dblp.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T21:39:38.729081Z",
     "start_time": "2018-01-31T21:39:38.601507Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 21:39:38,604 : INFO : saving Dictionary object under ../../data/data_dblp/corpora/dictionary.dblp.1997-2017, separately None\n",
      "2018-01-31 21:39:38,726 : INFO : saved ../../data/data_dblp/corpora/dictionary.dblp.1997-2017\n"
     ]
    }
   ],
   "source": [
    "dict_dblp.save(os.path.join(DATA_DIR, 'corpora', 'dictionary.dblp.1997-2017'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T21:41:57.265706Z",
     "start_time": "2018-01-31T21:41:56.896526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-31 21:41:56,899 : INFO : saving dictionary mapping to ../../data/data_dblp/corpora/dictionary.dblp.1997-2017.txt\n"
     ]
    }
   ],
   "source": [
    "dict_dblp.save_as_text(\n",
    "    os.path.join(DATA_DIR, 'corpora', 'dictionary.dblp.1997-2017.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-31T22:18:11.091062Z",
     "start_time": "2018-01-31T22:18:11.086676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_dblp.doc2bow(['we'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-01T00:12:27.029294Z",
     "start_time": "2018-02-01T00:07:43.162214Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, 'toks', 'bow2idx.all'), 'w') as f:\n",
    "    for pub in docs_dblp:\n",
    "        bow = dict_dblp.doc2bow(pub)\n",
    "        if len(bow):\n",
    "            rep = \"\".join([(str(a)+\" \")*c for (a, c) in bow])\n",
    "            f.write(rep.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowTotals <- apply(dtm , 1, sum)\n",
    "dtm.new   <- dtm[rowTotals> 0, ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "223px",
    "left": "1257px",
    "right": "108px",
    "top": "67.1333px",
    "width": "508px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "317px",
    "left": "1219px",
    "right": "106px",
    "top": "539px",
    "width": "548px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
